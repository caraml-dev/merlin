package logger

import (
	"errors"
	"fmt"
	"time"

	mlogs "github.com/caraml-dev/merlin/pkg/log"
	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
	"go.uber.org/zap"
	"google.golang.org/protobuf/proto"
)

type KafkaProducer interface {
	GetMetadata(*string, bool, int) (*kafka.Metadata, error)
	Produce(*kafka.Message, chan kafka.Event) error
}

const (
	// The largest record batch size allowed by Kafka (after compression if compression is enabled)
	MaxMessageBytes = 1048588
	// The compression type for all data generated by the Producer
	CompressionType = "none"
	// The maximum duration (ms) the Kafka Producer will block for to get Metadata, before timing out
	ConnectTimeoutMS = 1000
)

type KafkaSink struct {
	logger   *zap.SugaredLogger
	producer KafkaProducer

	topic        string
	serviceName  string
	projectName  string
	modelName    string
	modelVersion string
}

func NewKafkaSink(
	logger *zap.SugaredLogger,
	producer KafkaProducer,
	serviceName string,
	projectName string,
	modelName string,
	modelVersion string,
	topic string,
) LogSink {
	return &KafkaSink{
		logger:       logger,
		producer:     producer,
		topic:        topic,
		serviceName:  serviceName,
		projectName:  projectName,
		modelName:    modelName,
		modelVersion: modelVersion,
	}
}

func (k *KafkaSink) Sink(rawLogEntries []*LogEntry) error {
	// Format log inputs
	inferenceLogs, err := k.newLogMessages(rawLogEntries)
	if err != nil {
		return err
	}

	// A delivery channel for each message sent.
	// This permits to receive delivery reports
	// separately and to handle the use case
	// of a server that has multiple concurrent
	// produce requests and needs to deliver the replies
	// to many different response channels.
	deliveryChan := make(chan kafka.Event)
	for _, inferenceLog := range inferenceLogs {
		go func() {
			for e := range deliveryChan {
				switch ev := e.(type) {
				case *kafka.Message:
					// The message delivery report, indicating success or
					// permanent failure after retries have been exhausted.
					// Application level retries won't help since the client
					// is already configured to do that.
					m := ev
					if m.TopicPartition.Error != nil {
						k.logger.Errorf("Delivery failed: %v\n", m.TopicPartition.Error)
					} else {
						k.logger.Debugf("Delivered message to topic %s [%d] at offset %v\n",
							*m.TopicPartition.Topic, m.TopicPartition.Partition, m.TopicPartition.Offset)
					}
				default:
					k.logger.Infof("Ignored event: %s\n", ev)
				}
				// In this case the caller knows that this channel is used only
				// for one Produce call, so it can close it.
				close(deliveryChan)
			}
		}()

		keyBytes, valueBytes, err := k.buildNewKafkaMessage(inferenceLog)
		if err != nil {
			return err
		}

		err = k.producer.Produce(&kafka.Message{
			TopicPartition: kafka.TopicPartition{
				Topic:     &k.topic,
				Partition: kafka.PartitionAny},
			Value: valueBytes,
			Key:   keyBytes,
		}, deliveryChan)

		if err != nil {
			var kafkaError kafka.Error
			close(deliveryChan)
			if errors.As(err, &kafkaError) {
				if kafkaError.Code() == kafka.ErrQueueFull {
					// Producer queue is full, wait 1s for messages
					// to be delivered then try again.
					time.Sleep(time.Second)
					continue
				}
			}
			k.logger.Errorf("Failed to produce message: %v\n", err)
			return err
		}
	}

	return nil
}

func (k *KafkaSink) buildNewKafkaMessage(
	merlinLog *mlogs.InferenceLogMessage,
) (keyBytes []byte, valueBytes []byte, err error) {
	// Create the Kafka key
	key := &mlogs.InferenceLogKey{
		RequestId:      merlinLog.RequestId,
		EventTimestamp: merlinLog.EventTimestamp,
		ProjectName:    merlinLog.ProjectName,
		ModelName:      merlinLog.ModelName,
		ModelVersion:   merlinLog.ModelVersion,
	}

	// Marshal the key
	keyBytes, err = proto.Marshal(key)
	if err != nil {
		return nil, nil, fmt.Errorf("unable to marshal kafka key, %w", err)
	}

	// Marshal the message
	valueBytes, err = proto.Marshal(merlinLog)
	if err != nil {
		return nil, nil, fmt.Errorf("unable to marshal kafka value, %w", err)
	}

	return keyBytes, valueBytes, nil
}

func (k *KafkaSink) newLogMessages(logEntries []*LogEntry) ([]*mlogs.InferenceLogMessage, error) {
	messages := make([]*mlogs.InferenceLogMessage, 0)
	for _, logEntry := range logEntries {

		request := &mlogs.Request{}
		if logEntry.RequestPayload != nil {
			request = &mlogs.Request{
				Header: logEntry.RequestPayload.Headers,
				Body:   string(logEntry.RequestPayload.Body),
			}
		}

		response := &mlogs.Response{}
		if logEntry.ResponsePayload != nil {
			response = &mlogs.Response{
				StatusCode: int32(logEntry.ResponsePayload.StatusCode),
				Body:       string(logEntry.ResponsePayload.Body),
			}
		}

		logMessage := &mlogs.InferenceLogMessage{
			RequestId:      logEntry.RequestId,
			EventTimestamp: logEntry.EventTimestamp,
			ProjectName:    k.projectName,
			ModelName:      k.modelName,
			ModelVersion:   k.modelVersion,
			Request:        request,
			Response:       response,
		}

		messages = append(messages, logMessage)
	}

	return messages, nil
}
