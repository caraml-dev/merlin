{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BQ to BQ Batch Prediction Example: Predicting New York Taxi Trip Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Authenticated to gcloud (```gcloud auth application-default login```)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrate a more complex example of using batch prediction job in merlin. The example also demonstrate the scalability of merlin prediction job in processing a large amount of data (~150 Million rows).\n",
    "For basic introduction of batch prediction job in merlin you can read `Batch Prediction Tutorial 1 - Iris Classifier` notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "The problem that we are trying to solve in this notebook is to predict the total taxi fare of a taxi trip in new york city given following data:\n",
    "1. pickup_datetime\n",
    "2. pickup_longitude\n",
    "3. pickup_latitude\n",
    "4. dropoff_longitude\n",
    "5. dropoff_latitude\n",
    "6. passenger_count\n",
    "\n",
    "The data is available in BQ public dataset `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`. The table has 146,112,989 rows. We will train a model using a subset of data (50000 rows) and use the model to predict the whole table using merlin's batch prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download subset of the table for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "query_job = client.query(\"\"\"\n",
    "    SELECT\n",
    "      pickup_datetime,\n",
    "      pickup_longitude,\n",
    "      pickup_latitude,\n",
    "      dropoff_longitude,\n",
    "      dropoff_latitude,\n",
    "      passenger_count,\n",
    "      total_amount\n",
    "    FROM\n",
    "      `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`\n",
    "    LIMIT\n",
    "      50000\"\"\")\n",
    "\n",
    "results = query_job.result()\n",
    "df = results.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data to remove trip with: \n",
    "- 0 passenger_count \n",
    "- 0 latitude/longitude\n",
    "- Negative total_amount\n",
    "- Outside New York "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(0, np.nan).dropna()[df[\"total_amount\"] > 0.0]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_within_boundingbox(df, BB):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration\n",
    "    \"\"\"\n",
    "    return df[(df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n",
    "           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n",
    "           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n",
    "           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])]\n",
    "            \n",
    "# load image of NYC map\n",
    "BB = (-74.5, -72.8, 40.5, 41.8)\n",
    "df = select_within_boundingbox(df, BB)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "      \"pickup_datetime\",\n",
    "      \"pickup_longitude\",\n",
    "      \"pickup_latitude\",\n",
    "      \"dropoff_longitude\",\n",
    "      \"dropoff_latitude\",\n",
    "      \"passenger_count\"\n",
    "]\n",
    "label = \"total_amount\"\n",
    "\n",
    "X = df[features]\n",
    "y = df[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add transformation to:\n",
    "1. Process `pickup_datetime` into 3 additional features: `month`, `day_of_month`, `day_of_week` and `hour`\n",
    "2. Process the location features into distance features: `distance_haversine`and `distance_manhattan` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pickup_datetime(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day_of_month'] = df['pickup_datetime'].dt.day\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['day_of_week'] = df['pickup_datetime'].dt.dayofweek\n",
    "    \n",
    "def haversine_distance(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_distance(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_distance(lat1, lng1, lat2, lng1)\n",
    "    return a + b \n",
    "\n",
    "def transform(df):\n",
    "    process_pickup_datetime(df)\n",
    "    df[\"distance_haversine\"] = haversine_distance(\n",
    "            df['pickup_latitude'].values, \n",
    "            df['pickup_longitude'].values, \n",
    "            df['dropoff_latitude'].values, \n",
    "            df['dropoff_longitude'].values)\n",
    "    df[\"distance_manhattan\"] = manhattan_distance(\n",
    "                df['pickup_latitude'].values, \n",
    "                df['pickup_longitude'].values, \n",
    "                df['dropoff_latitude'].values, \n",
    "                df['dropoff_longitude'].values)\n",
    "    return df.drop(columns=['pickup_datetime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trans = transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_trans, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train an xgboost linear regressor with the training dataset and use RMSE to measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "model = xgb.XGBRegressor(max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "print(f\"Training RMSE: {math.sqrt(mean_squared_error(y_train, pred_train))}\")\n",
    "print(f\"Training MAE: {mean_absolute_error(y_train, pred_train)}\")\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "print(f\"Test RMSE: {math.sqrt(mean_squared_error(y_test, pred_test))}\")\n",
    "print(f\"Test MAE: {mean_absolute_error(y_test, pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model perform good enough, so let's use it predict the whole table (`bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015`) and store the result to `your-gcp-project.dataset.ny_taxi_prediction` table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wrap Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run batch prediction job we'll have to wrap the model inside a class implementing `PyFuncV2Model` abstract class. \n",
    "The class has 2 abstract method: `initialize` and `infer`:\n",
    "\n",
    "1. `initialize` is the entry point for initializing the model. Within this method you can do initialization step such as loading model from artifact. `initialize` will be called once during model initialization. The argument to initialize is a dictionary containing a key value pair of artifact name and its URL. The artifact's keys are the same value as received by `log_pyfunc_model`.\n",
    "2. `infer` method is the prediction method of your model. `infer` accept `pandas.DataFrame` as the input and should return either `np.ndarray`, `pd.Series`, or `pd.DataFrame` of same length.\n",
    "\n",
    "### IMPORTANT\n",
    "\n",
    "During batch prediction job execution, `infer` method will be called multiple times with different partition of the source data as the input. It is important that `infer` should avoid containing aggregation operation (e.g. mean, min, max) as the operation will only be applicable to the given partition, hence the result will be incorrect. If  aggregation is required, it is recommeded to do it outside of the prediction job and store the result as a column in the source table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will serialize the previously trained model using joblib, so that we can upload it as an artifact to merlin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"model\"\n",
    "MODEL_FILE = \"nyc-model.joblib\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n",
    "MODEL_PATH_ARTIFACT_KEY = \"model_path\" # we will use it when calling log_pyfunc_model\n",
    "\n",
    "joblib.dump(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create `NYTaxiFareModel` class extending `PyFuncV2Model` and implement the necessary methods: `initialize` and `infer`. \n",
    "\n",
    "In the `initialize` method, we load the serialized from `artifacts` key `MODEL_PATH_ARTIFACT_KEY` using joblib.\n",
    "\n",
    "In the `infer` method, we will apply tranformation to the table similarly as when we train the model (see: `transform` method above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from merlin.model import PyFuncV2Model\n",
    "\n",
    "class NYTaxiFareModel(PyFuncV2Model):    \n",
    "    def initialize(self, artifacts):\n",
    "        self.model = joblib.load(artifacts[MODEL_PATH_ARTIFACT_KEY])\n",
    "    \n",
    "    def infer(self, df_predict):\n",
    "        df = transform(df_predict)\n",
    "        return self.model.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NYTaxiFareModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.initialize({MODEL_PATH_ARTIFACT_KEY: MODEL_PATH})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = m.infer(X)\n",
    "print(f\"RMSE: {math.sqrt(mean_squared_error(y, pred))}\")\n",
    "print(f\"MAE: {mean_absolute_error(y, pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload To Merlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import merlin\n",
    "\n",
    "MERLIN_API_URL=\"http://localhost:3000/api/merlin\"\n",
    "\n",
    "merlin.set_url(MERLIN_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Set Active Project\n",
    "\n",
    "`project` represent a project in real life. You may have multiple model within a project.\n",
    "\n",
    "`merlin.set_project(<project_name>)` will set the active project into the name matched by argument. You can only set it to an existing project. If you would like to create a new project, please do so from the MLP console at http://localhost:3000/projects/create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merlin.set_project(\"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set Active Model\n",
    "\n",
    "`model` represents an abstract ML model. Conceptually, `model` in MLP is similar to a class in programming language. To instantiate a `model` you'll have to create a `model_version`.\n",
    "\n",
    "Each `model` has a type, currently model type supported by MLP are: sklearn, xgboost, tensorflow, pytorch, and user defined model (i.e. pyfunc model).\n",
    "\n",
    "`model_version` represents a snapshot of particular `model` iteration. You'll be able to attach information such as metrics and tag to a given `model_version` as well as deploy it as a model service.\n",
    "\n",
    "`merlin.set_model(<model_name>, <model_type>)` will set the active model to the name given by parameter, if the model with given name is not found, a new model will be created.\n",
    "\n",
    "Currently, batch prediction job is only supported by `PYFUNC_V2` model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.model import ModelType\n",
    "\n",
    "merlin.set_model(\"nyc-batch\", ModelType.PYFUNC_V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create New Model Version And Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the model, we will have to create an iteration of the model (by creating a `model_version`), upload the serialized model to MLP, and then deploy.\n",
    "\n",
    "To upload PyFunc model you have to provide following arguments:\n",
    "1. `model_instance` is the instance of PyFunc model, the model has to extend `merlin.PyFuncModel` or `merlin.PyFuncModelV2`\n",
    "2. `conda_env` is path to conda environment yaml file. The environment yaml file must contain all dependency required by the PyFunc model.\n",
    "3. (Optional) `artifacts` is additional artifact that you want to include in the model\n",
    "4. (Optional) `code_path` is a list of directory containing python code that will be loaded during model initialization, this is required when `model_instance` depend on local python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new version of the model\n",
    "with merlin.new_model_version() as v:\n",
    "    # Upload the serialized model to MLP\n",
    "    merlin.log_pyfunc_model(model_instance=m, \n",
    "                            conda_env=\"env.yaml\", \n",
    "                            artifacts={MODEL_PATH_ARTIFACT_KEY: MODEL_PATH})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check whether the model has been uploaded successfully by opening the model version's mlflow url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.mlflow_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Batch Prediction Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch prediction job will use `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015` as data source and \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"passenger_count\" as features. The prediction result will be stored in `your-gcp-project.dataset.ny_taxi_prediction` table under `total_fare` column. \n",
    "\n",
    "Since the data size is quite large, we will not use default resource request and instead specify the request using `PredictionJobResourceRequest` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.batch.source import BigQuerySource\n",
    "from merlin.batch.sink import BigQuerySink, SaveMode\n",
    "from merlin.batch.config import PredictionJobConfig, PredictionJobResourceRequest\n",
    "\n",
    "SOURCE_TABLE = \"bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2015\"\n",
    "SINK_TABLE=\"gcp-project.dataset.ny_taxi_prediction\"\n",
    "SINK_STAGING_BUCKET=\"gcs-bucket\"\n",
    "SERVICE_ACCOUNT_NAME=\"batch-service-account@gcp-project.iam.gserviceaccount.com\"\n",
    "\n",
    "bq_source = BigQuerySource(SOURCE_TABLE,\n",
    "                           features=[ \"pickup_datetime\",\n",
    "                                      \"pickup_longitude\",\n",
    "                                      \"pickup_latitude\",\n",
    "                                      \"dropoff_longitude\",\n",
    "                                      \"dropoff_latitude\",\n",
    "                                      \"passenger_count\"])\n",
    "\n",
    "bq_sink = BigQuerySink(SINK_TABLE,\n",
    "                       staging_bucket=SINK_STAGING_BUCKET,\n",
    "                       result_column=\"total_fare\",\n",
    "                       save_mode=SaveMode.OVERWRITE)\n",
    "\n",
    "job_config = PredictionJobConfig(source=bq_source, \n",
    "                                 sink=bq_sink, \n",
    "                                 service_account_name=SERVICE_ACCOUNT_NAME,\n",
    "                                resource_request=PredictionJobResourceRequest(driver_cpu_request=\"1\",\n",
    "                                                    driver_memory_request=\"1Gi\",\n",
    "                                                    executor_cpu_request=\"2\",\n",
    "                                                    executor_memory_request=\"2Gi\",\n",
    "                                                    executor_replica=6))\n",
    "\n",
    "job = v.create_prediction_job(job_config=job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once, the prediction job has been completed we can check the result in destination table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(\"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `your-gcp-project.dataset.ny_taxi_prediction`\n",
    "    LIMIT\n",
    "      100\"\"\")\n",
    "\n",
    "results = query_job.result()\n",
    "df = results.to_dataframe()\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('merlin-sdk': pipenv)",
   "language": "python",
   "name": "python37064bitmerlinsdkpipenv4227646db3a047c9bf0abdc83ce0196a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
